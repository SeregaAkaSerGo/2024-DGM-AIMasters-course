{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyfGcNY4pcXd"
   },
   "source": [
    "# Homework3: Variational inference and VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvqXOhwQf0c2"
   },
   "source": [
    "## Task 1: Theory (4pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07321QwCf0c3"
   },
   "source": [
    "### Problem 1: IWAE theory (1.5pt)\n",
    "\n",
    "Variational inference is based on the ELBO objective:\n",
    "$$\n",
    "    \\mathcal{L} (\\boldsymbol{\\phi}, \\boldsymbol{\\theta})  = \\mathbb{E}_{\\mathbf{z} \\sim q(\\mathbf{z} | \\mathbf{x})} \\log \\left( \\frac{p(\\mathbf{x}, \\mathbf{z} | \\boldsymbol{\\theta})}{q(\\mathbf{z}| \\mathbf{x}, \\boldsymbol{\\phi})} \\right) \\rightarrow \\max_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}.\n",
    "$$\n",
    "ELBO is a lower bound of the log-likelihood. However if the gap between ELBO and log-likelihood is large, then our model is not optimal. In this task we discuss the way, how to improve the lower bound.\n",
    "\n",
    "The improvement was introduced in the [IWAE](https://arxiv.org/abs/1509.00519) model. This model introduces the improved version of the variational lower bound (ELBO):\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}_K (\\boldsymbol{\\phi}, \\boldsymbol{\\theta}) = \\mathbb{E}_{\\mathbf{z}_1, \\dots, \\mathbf{z}_K \\sim q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})} \\log \\left( \\frac{1}{K}\\sum_{k=1}^K\\frac{p(\\mathbf{x}, \\mathbf{z}_k | \\boldsymbol{\\theta})}{q(\\mathbf{z}_k| \\mathbf{x}, \\boldsymbol{\\phi})} \\right) \\rightarrow \\max_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}}.\n",
    "$$\n",
    "\n",
    "Note, that the difference with the original ELBO is the sum over $K$ different latent vectors $\\mathbf{z}_k$.\n",
    "\n",
    "Moreover we can rewrite ELBO in the following form:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L} (\\boldsymbol{\\phi}, \\boldsymbol{\\theta}) = \\frac{1}{K} \\sum_{k=1}^K\\mathcal{L} (\\boldsymbol{\\phi}, \\boldsymbol{\\theta}) = \\frac{1}{K} \\sum_{k=1}^K \\mathbb{E}_{\\mathbf{z}_k \\sim q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})} \\log \\left( \\frac{p(\\mathbf{x}, \\mathbf{z}_k | \\boldsymbol{\\theta})}{q(\\mathbf{z}_k| \\mathbf{x}, \\boldsymbol{\\phi})} \\right) = \\frac{1}{K}  \\mathbb{E}_{\\mathbf{z}_k \\sim q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})} \\sum_{k=1}^K \\log \\left( \\frac{p(\\mathbf{x}, \\mathbf{z}_k | \\boldsymbol{\\theta})}{q(\\mathbf{z}_k| \\mathbf{x}, \\boldsymbol{\\phi})} \\right).\n",
    "$$\n",
    "Here we see that the only difference between these two objectives ($\\mathcal{L} (\\boldsymbol{\\phi}, \\boldsymbol{\\theta})$ and $\\mathcal{L}_K (\\boldsymbol{\\phi}, \\boldsymbol{\\theta})$) is the order of sum and logarithm.\n",
    "\n",
    "Our task here is two proof that the objective $\\mathcal{L}_K (\\boldsymbol{\\phi}, \\boldsymbol{\\theta})$ is also a lower bound of log-likelihood and this lower bound is better than the initial ELBO.\n",
    "\n",
    "We have to prove the following facts:\n",
    "\n",
    "1. $\\log p(\\mathbf{x} | \\boldsymbol{\\theta}) \\geq \\mathcal{L}_K (\\boldsymbol{\\phi}, \\boldsymbol{\\theta}) \\geq \\mathcal{L}_M (\\boldsymbol{\\phi}, \\boldsymbol{\\theta}), \\quad \\text{for } K \\geq M$;\n",
    "2.  $\\log p(\\mathbf{x} | \\boldsymbol{\\theta}) = \\lim_{K \\rightarrow \\infty} \\mathcal{L}_K (\\boldsymbol{\\phi}, \\boldsymbol{\\theta})$ if $\\frac{p(\\mathbf{x}, \\mathbf{z} | \\boldsymbol{\\theta})}{q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})}$ is bounded.\n",
    "\n",
    "**Hints:**\n",
    "1. First part of the theorem.\n",
    "\n",
    "    (a) Use the following equation inside the logarithm of $\\mathcal{L}_K (q, \\boldsymbol{\\theta})$\n",
    "$$\n",
    "    \\frac{a_1 + \\dots + a_K}{K} = \\mathbb{E}_{k_1, \\dots, k_M} \\frac{a_{k_1} + \\dots + a_{k_M}}{M}, \\quad k_1, \\dots, k_M \\sim U[1, K]\n",
    "$$\n",
    "    (b) Apply Jensen' inequality.\n",
    "3. Second part of the theorem: use the Law of large numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3J37lldMf0c3"
   },
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqU9vsu2CYLB"
   },
   "source": [
    "### Problem 1: ELBO surgery (1pt)\n",
    "\n",
    "At the lecture 6 we proved the [ELBO surgery](http://approximateinference.org/accepted/HoffmanJohnson2016.pdf) theorem:\n",
    "$$\n",
    "    \\frac{1}{n} \\sum_{i=1}^n KL(q(\\mathbf{z} | \\mathbf{x}_i) || p(\\mathbf{z})) = KL(q_{\\text{agg}}(\\mathbf{z}) || p(\\mathbf{z})) + \\mathbb{I}_{q} [\\mathbf{x}, \\mathbf{z}],\n",
    "$$\n",
    "where the first term is $KL(q_{\\text{agg}}(\\mathbf{z}) || p(\\mathbf{z}))$ includes the aggregated posterior distribution $q_{\\text{agg}}(\\mathbf{z})$ and the prior distribution $p(\\mathbf{z})$. Our goal now is to deal with the second term. At the lecture, the second term was equal to:\n",
    "\n",
    "$$\n",
    "    \\mathbb{I}_{q} [\\mathbf{x}, \\mathbf{z}] = \\frac{1}{n}\\sum_{i=1}^n KL(q(\\mathbf{z} | \\mathbf{x}_i) || q_{\\text{agg}}(\\mathbf{z})).\n",
    "$$\n",
    "In fact, this is a mutual information between $\\mathbf{x}$ and $\\mathbf{z}$ on the empirical distribution of data and the distribution of $q(\\mathbf{z} | \\mathbf{x})$. Let treat the index of the sample $i$ as a random variable.\n",
    "$$\n",
    "    q(i, \\mathbf{z}) = q(i) q(\\mathbf{z} | i); \\quad p(i, \\mathbf{z}) = p(i) p(\\mathbf{z}); \\quad\n",
    "    q(i) = p(i) = \\frac{1}{n}.\n",
    "$$\n",
    "$$\n",
    "    \\quad q(\\mathbf{z} | i) = q(\\mathbf{z} | \\mathbf{x}_i) \\quad q_{\\text{agg}}(\\mathbf{z}) = \\sum_{i=1}^n q(i, \\mathbf{z}) = \\frac{1}{n} \\sum_{i=1}^n q(\\mathbf{z} | \\mathbf{x}_i);  \n",
    "$$\n",
    "Mutual information is a measure of independence between two random variables.\n",
    "$$\n",
    "\t\\mathbb{I}_{q} [\\mathbf{x}, \\mathbf{z}] = \\mathbb{E}_{q(i, \\mathbf{z})} \\log \\frac{q(i, \\mathbf{z})}{q(i)q_{\\text{agg}}(\\mathbf{z})}.\n",
    "$$\n",
    "Prove that 2 expressions for mutual information are equal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1uu5MOXCaoj"
   },
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-E6G4FACeJM"
   },
   "source": [
    "### Problem 3: Gumbel-Max trick (1.5pt)\n",
    "\n",
    "In this problem you have to prove the Gumbel-Max trick that we have discussed at the Lecture 6.\n",
    "\n",
    "Let $\\pi_1, \\pi_2, \\dots \\pi_K, \\in (0, 1)$ and $\\sum\\limits_{k = 1}^{K} \\pi_k = 1$. Consider the discrete random variable:\n",
    "\n",
    "$$\n",
    "  c = \\arg\\max_{k} \\left[\\log \\pi_k + g_k\\right].\n",
    "$$\n",
    "\n",
    "In the formula above $g_k$ ($k \\in \\{1, \\dots K\\}$) are independent random variables distributed following the $\\text{Gumbel}(0, 1)$ distribution ([wiki](https://en.wikipedia.org/wiki/Gumbel_distribution)), i.e. $g_k \\sim \\text{Gumbel}(0, 1)$.\n",
    "\n",
    "Note that $g_k = - \\log (- \\log u)$, where $u \\sim \\text{Uniform}[0, 1]$.\n",
    "\n",
    "Our goal is to prove that $c \\sim \\text{Categorical}(\\pi_1, \\dots \\pi_K)$.\n",
    "\n",
    "1. Find cumulative distribution function ($F_{g}(x) = P(g < x)$) of Gumbel distribution.\n",
    "\n",
    "2. Find density of the Gumbel distribution (derivative of cdf).\n",
    "\n",
    "3. Consider random variables $\\zeta_k = \\log \\pi_k + g_k$. Let's fix $k^* \\in \\{1, \\dots K\\}$ and look at the following probability $P\\bigl( \\{\\zeta_{k} \\leq \\zeta_{k^*}\\} \\text{ for all } k \\neq k^*\\bigr)$. Prove that\n",
    "\n",
    "$$\n",
    "  P\\bigl( \\bigcap\\limits_{k \\neq k^*} \\{\\zeta_{k} \\leq \\zeta_{k^*}\\}\\bigr) = \\pi_{k^*}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AO8IR1lKChKE"
   },
   "source": [
    "```your solution```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32394,
     "status": "ok",
     "timestamp": 1709365395802,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "9leRbWNBqZM2",
    "outputId": "1cc319b8-c194-4385-f28c-dfaacc7a4bb4"
   },
   "outputs": [],
   "source": [
    "REPO_NAME = \"2024-DGM-AIMasters-course\"\n",
    "!if [ -d {REPO_NAME} ]; then rm -Rf {REPO_NAME}; fi\n",
    "!git clone https://github.com/r-isachenko/{REPO_NAME}.git\n",
    "!cd {REPO_NAME}\n",
    "!pip install ./{REPO_NAME}/homeworks/\n",
    "!rm -Rf {REPO_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8793,
     "status": "ok",
     "timestamp": 1709365404592,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "Nc5RAWFOqekr"
   },
   "outputs": [],
   "source": [
    "from dgm_utils import train_model, plot_training_curves\n",
    "from dgm_utils import show_samples, visualize_images, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1709365404592,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "EIBqEphlrEGd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Optional\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN3th9zXANgx"
   },
   "source": [
    "## Task 2: ResNetVAE on CIFAR10 data (4pt)\n",
    "\n",
    "In this task you will implement VAE model for CIFAR10 dataset.\n",
    "\n",
    "Let download and visualize samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "executionInfo": {
     "elapsed": 10433,
     "status": "ok",
     "timestamp": 1709365415014,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "POBb7efUAQhp",
    "outputId": "0324c89f-b8a8-4b29-effb-b21fb8579b21"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(\"cifar10\", flatten=False, binarize=False)\n",
    "visualize_images(train_data, \"CIFAR10 samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biYy9_rWd-DY"
   },
   "source": [
    "Now it is time to define our model. Our model will have the following structure:\n",
    "\n",
    "* Prior distribution is standard Normal ($p(\\mathbf{z}) = \\mathcal{N}(0, I)$).\n",
    "* Variational posterior distribution (or encoder) is $q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x}))$. Here $\\boldsymbol{\\phi}$ denotes all parameters of the encoder neural network. We will assume that covariance matrice $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x})$ is diagonal.\n",
    "* Generative distribution (or decoder) is $p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))$. Here $\\boldsymbol{\\theta}$ denotes all parameters of the decoder neural network. Please note, that here we will use continuous distribution for our variables $\\mathbf{x}$.\n",
    "* We do not fit the covariance matrix $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z})$ in the generative distribution $p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})$. We assume that it is identical ($\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}) = \\mathbf{I}$). We will use the $\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z})$ (mean of the generative distribution $p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})$) as model samples.\n",
    "* Our encoder and decoder will be convolutional neural networks.\n",
    "* Model objective is slightly modified ELBO:\n",
    "$$\n",
    "    \\mathcal{L}(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}) = \\mathbb{E}_{q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})} \\log p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) - \\beta * KL (q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) || p(\\mathbf{z})).\n",
    "$$\n",
    "Here we introduce the parameter $\\beta$. It reweights KL term in the total loss. It a standard heuristics that allows to get more accurate model. In this exercise you have to play with it, starting with the value $\\beta = 1$ (standard ELBO).\n",
    "\n",
    "To make the expectation is independent of parameters $\\boldsymbol{\\phi}$, we will use reparametrization trick.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxqxyXZBbKfy"
   },
   "source": [
    "To calculate the loss, we should derive\n",
    "- $\\log p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})$, note that generative distribution is $\\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))$.\n",
    "- KL between $q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x}))$ and $\\mathcal{N}(0, \\mathbf{I})$.\n",
    "\n",
    "Let start with the helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1709365415014,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "_ZEUyHuxE39I"
   },
   "outputs": [],
   "source": [
    "def get_normal_KL(\n",
    "    mean_1: torch.Tensor,\n",
    "    log_std_1: torch.Tensor,\n",
    "    mean_2: Optional[torch.Tensor] = None,\n",
    "    log_std_2: Optional[torch.Tensor] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :Parameters:\n",
    "    mean_1: means of normal distributions (1)\n",
    "    log_std_1 : standard deviations of normal distributions (1)\n",
    "    mean_2: means of normal distributions (2)\n",
    "    log_std_2 : standard deviations of normal distributions (2)\n",
    "    :Outputs:\n",
    "    kl divergence of the normal distributions (1) and normal distributions (2)\n",
    "    ---\n",
    "    This function should return the value of KL(p1 || p2),\n",
    "    where p1 = Normal(mean_1, exp(log_std_1) ** 2), p2 = Normal(mean_2, exp(log_std_2) ** 2).\n",
    "    If mean_2 and log_std_2 are None values, we will use standard normal distribution.\n",
    "    Note that we consider the case of diagonal covariance matrix.\n",
    "    \"\"\"\n",
    "    if mean_2 is None:\n",
    "        mean_2 = torch.zeros_like(mean_1)\n",
    "    if log_std_2 is None:\n",
    "        log_std_2 = torch.zeros_like(log_std_1)\n",
    "    assert mean_1.shape == log_std_1.shape == mean_2.shape == log_std_2.shape\n",
    "    # ====\n",
    "    # your code\n",
    "    \n",
    "    # ====\n",
    "\n",
    "\n",
    "def test_KL():\n",
    "    assert np.isclose(\n",
    "        get_normal_KL(\n",
    "            torch.tensor(2), torch.tensor(3), torch.tensor(0), torch.tensor(0)\n",
    "        ).numpy(),\n",
    "        200.2144,\n",
    "        rtol=1e-3,\n",
    "    )\n",
    "    assert np.isclose(\n",
    "        get_normal_KL(\n",
    "            torch.tensor(2), torch.tensor(3), torch.tensor(4), torch.tensor(5)\n",
    "        ).numpy(),\n",
    "        1.50925,\n",
    "        rtol=1e-3,\n",
    "    )\n",
    "    assert np.allclose(\n",
    "        get_normal_KL(\n",
    "            torch.tensor((10, 10)), torch.tensor((2, 4)), torch.tensor((3, 5))\n",
    "        ).numpy(),\n",
    "        [49.2990, 1498.479],\n",
    "        rtol=1e-3,\n",
    "    )\n",
    "\n",
    "\n",
    "test_KL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1709365415014,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "871Pfpm1TiWF"
   },
   "outputs": [],
   "source": [
    "def get_normal_nll(\n",
    "    x: torch.Tensor, mean: torch.Tensor, log_std: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This function should return the negative log likelihood log p(x),\n",
    "    where p(x) = Normal(x | mean, exp(log_std) ** 2).\n",
    "    Note that we consider the case of diagonal covariance matrix.\n",
    "    \"\"\"\n",
    "    # ====\n",
    "    # your code\n",
    "    \n",
    "    # ====\n",
    "\n",
    "\n",
    "def test_NLL():\n",
    "    assert np.isclose(\n",
    "        get_normal_nll(torch.tensor(2), torch.tensor(2), torch.tensor(3)).numpy(),\n",
    "        3.9189,\n",
    "        rtol=1e-3,\n",
    "    )\n",
    "    assert np.isclose(\n",
    "        get_normal_nll(torch.tensor(5), torch.tensor(-3), torch.tensor(6)).numpy(),\n",
    "        6.9191,\n",
    "        rtol=1e-3,\n",
    "    )\n",
    "    assert np.allclose(\n",
    "        get_normal_nll(\n",
    "            torch.tensor((10, 10)), torch.tensor((2, 4)), torch.tensor((3, 5))\n",
    "        ).numpy(),\n",
    "        np.array([3.9982, 5.9197]),\n",
    "        rtol=1e-3,\n",
    "    )\n",
    "\n",
    "\n",
    "test_NLL()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ7cIAhkfhif"
   },
   "source": [
    "Let define our encoder and decoder neural networks. We will use ResNet-like encoder and decoder.\n",
    "\n",
    "First of all let define basic ResNet block. It will be the basic block for our encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14725,
     "status": "ok",
     "timestamp": 1709365429734,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "SBZA5vBwf0c-"
   },
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, final_relu=True):\n",
    "        super().__init__()\n",
    "        self.final_relu = final_relu\n",
    "        # ====\n",
    "        # your code\n",
    "        # here you could try different network structures\n",
    "        # we suggest to use the following:\n",
    "        # residual(x) = conv(bn(relu(conv(bn(x)))))\n",
    "        # output = relu(conv1x1(input) + residual(input))\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        \n",
    "        # ====\n",
    "        if self.final_relu:\n",
    "            result = self.relu2(result)\n",
    "        return result\n",
    "\n",
    "\n",
    "def test_resnet_block():\n",
    "    test_inp = torch.randn(5, 64, 128, 128)\n",
    "\n",
    "    for out_channels in [64, 128]:\n",
    "        for kernel_size in [3, 5, 7]:\n",
    "            for stride in [1, 2, 4]:\n",
    "                resnet_block = ResNetBlock(in_channels=64, out_channels=out_channels,\n",
    "                                        kernel_size=kernel_size, stride=stride)\n",
    "                assert list(resnet_block(test_inp).shape) == [5, out_channels, 128 // stride, 128 // stride]\n",
    "\n",
    "\n",
    "test_resnet_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5njl8CQpvqw"
   },
   "source": [
    "Now let define basic ResNet block. It will be the basic block for our decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35258,
     "status": "ok",
     "timestamp": 1709365464989,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "19M3VevKYCrn"
   },
   "outputs": [],
   "source": [
    "class ResNetTransposeBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, final_relu=True):\n",
    "        super().__init__()\n",
    "        self.final_relu = final_relu\n",
    "        # ====\n",
    "        # your code\n",
    "        # here you could try different network structures\n",
    "        # we suggest to use the following:\n",
    "        # output = conv(bn(f(input))) + f(input), where:\n",
    "        # f(x) = upconv(bn(x))\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        \n",
    "        # ====\n",
    "        if self.final_relu:\n",
    "            result = self.relu1(result)\n",
    "        return result\n",
    "\n",
    "\n",
    "def test_resnet_transposed_block():\n",
    "    test_inp = torch.randn(5, 64, 64, 64)\n",
    "    for out_channels in [64, 128]:\n",
    "        for kernel_size in [4, 6, 8]:\n",
    "            for stride in [2, 4]:\n",
    "                resnet_block = ResNetTransposeBlock(\n",
    "                    in_channels=64, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "                assert list(resnet_block(test_inp).shape) == [5, out_channels, 64 * stride, 64 * stride]\n",
    "\n",
    "\n",
    "test_resnet_transposed_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAYKzNCqp2o-"
   },
   "source": [
    "Now we are to define our encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1709365464989,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "jFkDOrg7YVrB"
   },
   "outputs": [],
   "source": [
    "class ConvResNetEncoder(nn.Module):\n",
    "    def __init__(self, input_shape, n_latent):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "        # ====\n",
    "        # your code\n",
    "        # our suggestions:\n",
    "        # - try to combine multiple resnet blocks\n",
    "        # - place Flatten + Linear at the end\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply all blocks defined in init\n",
    "        # split output tensor to mu and log_std\n",
    "        \n",
    "        # ====\n",
    "        return mu, log_std\n",
    "\n",
    "\n",
    "class ConvResNetDecoder(nn.Module):\n",
    "    def __init__(self, n_latent, output_shape):\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "        self.output_shape = output_shape\n",
    "        # ====\n",
    "        # your code\n",
    "        # our suggestions:\n",
    "        # - apply linear layer to the input\n",
    "        # - reshape output matrix to 4-dims tensor\n",
    "        # - try to combine multiple resnet transposed blocks\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def forward(self, z):\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply all blocks defined in init\n",
    "\n",
    "        # ====\n",
    "        return out\n",
    "\n",
    "\n",
    "def test_convresnet_models():\n",
    "    test_enc = ConvResNetEncoder((3, 32, 32), n_latent=10)\n",
    "    inp = torch.randn((4, 3, 32, 32))\n",
    "    mu, std = test_enc(inp)\n",
    "\n",
    "    assert list(mu.shape) == [4, 10]\n",
    "    assert list(std.shape) == [4, 10]\n",
    "\n",
    "    test_dec = ConvResNetDecoder(10, (3, 32, 32))\n",
    "    inp = torch.randn(4, 10)\n",
    "    assert list(test_dec(inp).shape) == [4, 3, 32, 32]\n",
    "\n",
    "\n",
    "test_convresnet_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXtOjEvbftM9"
   },
   "source": [
    "We are ready to implement VAE model for image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1709365464989,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "v3T4RRzUfrdg"
   },
   "outputs": [],
   "source": [
    "class ConvResNetVAE(nn.Module):\n",
    "    def __init__(self, input_shape: tuple, n_latent: int, beta: float = 1) -> None:\n",
    "        super().__init__()\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "        self.beta = beta\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def prior(self, n: int) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # return n samples from prior distribution (we use standart normal for prior)\n",
    "        \n",
    "        # ====\n",
    "\n",
    "        z = z.to(self.device)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply encoder to get mu_z, log_std_z\n",
    "        # 2) apply reparametrization trick (use self.prior)\n",
    "        # 3) apply decoder to get mu_x (which corresponds to reconstructed x)\n",
    "        \n",
    "        # ====\n",
    "        return mu_z, log_std_z, x_recon\n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> dict:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) make forward step to get mu_z, log_std_z, x_recon\n",
    "        # 2) calculate recon_loss (use get_normal_nll)\n",
    "        # 3) calcucalte kl_loss (use get_normal_KL)\n",
    "        \n",
    "        # ====\n",
    "        return {\n",
    "            \"elbo_loss\": recon_loss + self.beta * kl_loss,\n",
    "            \"recon_loss\": recon_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        with torch.no_grad():\n",
    "            # ====\n",
    "            # your code\n",
    "            # 1) generate prior samples\n",
    "            # 2) apply decoder\n",
    "            \n",
    "            # ====\n",
    "            samples = torch.clamp(x_recon, -1, 1)\n",
    "        return samples.cpu().numpy() * 0.5 + 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxeq_hyUqzuk"
   },
   "source": [
    "That is all! We are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547,
     "referenced_widgets": [
      "7a5d33c83dac4086a8cd16b226b59dfd",
      "159726388b174e5caa99596a7dba1757",
      "dc7e4461995942a9855567ee63808372",
      "93cf61378d324a7498665c969526ce92",
      "9b97df51f0764ba09eebd9fb8cac9380",
      "c9928da49da140ee9f7c647d9c4dd25b",
      "e139d57637af473e9254cdbb5da91fc2",
      "dc8fc8cbe5e949e6a279f2692cff54aa",
      "65082ee43a844f62ac8287b8faef5b66",
      "a4763079e73a4982a540c05cd82a00d2",
      "78cb849d96014a108015bd3f92f2b968"
     ]
    },
    "executionInfo": {
     "elapsed": 221886,
     "status": "ok",
     "timestamp": 1709365686873,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "vr9PNknYaLTx",
    "outputId": "5bd12e64-cfa1-44d6-f9d8-6083ffe4ec45"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "\n",
    "BATCH_SIZE =   # any adequate value\n",
    "EPOCHS =   # < 16\n",
    "LR =   # < 1e-3\n",
    "N_LATENS =   # 128 < _ < 1024\n",
    "BETA =   # 0.1 < _ < 10\n",
    "# ====\n",
    "\n",
    "# we center the data, because it helps the model to fit\n",
    "centered_train_data = train_data * 2 - 1\n",
    "centered_test_data = test_data * 2 - 1\n",
    "\n",
    "train_loader = data.DataLoader(centered_train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(centered_test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = ConvResNetVAE((3, 32, 32), N_LATENS, BETA)\n",
    "\n",
    "train_losses, test_losses = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    loss_key=\"elbo_loss\",\n",
    "    use_tqdm=True,\n",
    "    use_cuda=USE_CUDA,\n",
    ")\n",
    "for key, value in test_losses.items():\n",
    "    print(\"{}: {:.4f}\".format(key, value[-1]))\n",
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlE8JlD-f1B9"
   },
   "source": [
    "Now we could visualize the model outputs.\n",
    "\n",
    "1. We could sample new images from our model (sample latent variable from the prior and apply the decoder).\n",
    "2. We could visualize image reconstructions (apply the encoder and the decoder to the fixed image).\n",
    "3. Visualize interpolations (apply the encoder to two images $\\mathbf{x}_1$ and $\\mathbf{x}_2$ to obtain the latent variables $\\mathbf{z}_1$ and $\\mathbf{z}_2$, apply the decoder to the latent variables $\\mathbf{z}$ lying on the segment between $\\mathbf{z}_1$ and $\\mathbf{z}_2$).\n",
    "\n",
    "**Note:** it is ok that your samples are blurry. We do not use difficult architectures and do not tune hyperparameters carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2319,
     "status": "ok",
     "timestamp": 1709365689190,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "GYQWcZ02aq9X",
    "outputId": "054eccc3-4898-4590-d930-8794ba486356"
   },
   "outputs": [],
   "source": [
    "samples = model.sample(100)\n",
    "\n",
    "x = next(iter(test_loader))[:50]\n",
    "\n",
    "x = x.to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z, _ = model.encoder(x)\n",
    "    x_recon = torch.clamp(model.decoder(z), -1, 1)\n",
    "reconstructions = torch.stack((x, x_recon), dim=1).view(-1, 3, 32, 32) * 0.5 + 0.5\n",
    "reconstructions = reconstructions.cpu().numpy()\n",
    "\n",
    "x = next(iter(test_loader))[:20]\n",
    "x = x.to(model.device)\n",
    "with torch.no_grad():\n",
    "    z, _ = model.encoder(x)\n",
    "    z1, z2 = z.chunk(2, dim=0)\n",
    "    interps = [model.decoder(z1 * (1 - alpha) + z2 * alpha) for alpha in np.linspace(0, 1, 10)]\n",
    "    interps = torch.stack(interps, dim=1).view(-1, 3, 32, 32)\n",
    "    interps = torch.clamp(interps, -1, 1) * 0.5 + 0.5\n",
    "interps = interps.cpu().numpy()\n",
    "\n",
    "show_samples(reconstructions, 'CIFAR10 reconstructions')\n",
    "show_samples(samples, 'CIFAR10 samples')\n",
    "show_samples(interps, 'CIFAR10 interpolation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3NaO2WkHdBo"
   },
   "source": [
    "## Task 3: VQ-VAE on MNIST (5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLFM5BtCHhDf"
   },
   "source": [
    "### Training of VQ-VAE model\n",
    "\n",
    "In this part you will train [VQ-VAE](https://arxiv.org/abs/1711.00937) model that we have discussed at the Lecture 6 (see also [VQ-VAE-2](https://arxiv.org/abs/1906.00446) paper).\n",
    "\n",
    "We will you MNIST dataset in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 900
    },
    "executionInfo": {
     "elapsed": 1920,
     "status": "ok",
     "timestamp": 1709365695756,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "HmOaQgtGHfTm",
    "outputId": "cd84259c-7020-4f61-f76a-9960bd370dcf"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(\"mnist\", flatten=False, binarize=True)\n",
    "visualize_images(train_data, \"MNIST samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJER2s8rHmXT"
   },
   "source": [
    "VQ-VAE model is a VAE model with discrete latent variable.  \n",
    "\n",
    "**Reminder:**\n",
    "* We define  dictionary (word book) space $\\{\\mathbf{e}_k\\}_{k=1}^K$, where $\\mathbf{e}_k \\in \\mathbb{R}^C$, $K$ is the size of the dictionary.\n",
    "* $\\mathbf{z}_e = \\text{NN}_e(\\mathbf{x}, \\boldsymbol{\\phi})$ - continuous output of encoder network.\n",
    "* $\\mathbf{z}_q = \\mathbf{e}_{k^*}$ is a quantized representation, where $k^* = \\text{argmin}_k \\| \\mathbf{z} - \\mathbf{e}_k \\|$. It is simple nearest neighbor look up.\n",
    "* Out deterministic variational posterior:\n",
    "$$\n",
    "  q(c = k^* | \\mathbf{x}, \\boldsymbol{\\phi}) = \\begin{cases}\n",
    "  1 , \\quad \\text{for } k^* = \\text{argmin}_k \\| \\mathbf{z}_e - \\mathbf{e}_k \\|; \\\\\n",
    "  0, \\quad \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "* Prior distribution is uniform: $p(c) = \\text{Uniform}\\{1, \\dots, K\\}$.\n",
    "* KL divergence between posterior and prior:\n",
    "$$\n",
    "  KL(q(c = k^* | \\mathbf{x}, \\boldsymbol{\\phi}), p(c)) = \\log K.\n",
    "$$\n",
    "* ELBO:\n",
    "$$\n",
    "\t\t\\mathcal{L} (\\boldsymbol{\\phi}, \\boldsymbol{\\theta})  = \\mathbb{E}_{q(c | \\mathbf{x}, \\boldsymbol{\\phi})} \\log p(\\mathbf{x} | \\mathbf{e}_{c} , \\boldsymbol{\\theta}) - \\log K =  \\log p(\\mathbf{x} | \\mathbf{z}_q, \\boldsymbol{\\theta}) - \\log K.\n",
    "$$\n",
    "* Vector quantization is non-differentiable operation. We will use **straight-through** gradient estimator (we will copy gradients from decoder input $\\mathbf{z}_q$ to encoder output $\\mathbf{z}_e$.\n",
    "\n",
    "**Important modifications:**\n",
    "Due to the straight-through gradient estimation of mapping from $\\mathbf{z}_e$ to $\\mathbf{z}_q$, the embeddings $\\mathbf{e}$ receive no gradients from the ELBO.\n",
    "\n",
    "Therefore, in order to learn the embedding space we add l2 loss (**codebook loss**) to move the embedding vectors $\\mathbf{e}$ towards the encoder outputs $\\mathbf{z}_e$.\n",
    "\n",
    "Finally, since the volume of the embedding space is dimensionless, it can grow arbitrarily if the embeddings $\\mathbf{e}$ do not train as fast as the encoder parameters. To make sure the encoder commits to an embedding and its output does not grow, we add a **commitment loss**.\n",
    "\n",
    "Thus, the total training objective becomes:\n",
    "$$\n",
    "  \\log p(\\mathbf{x}| \\mathbf{z}_q, \\boldsymbol{\\theta}) + \\| \\text{stop\\_gradient}(\\mathbf{z}_e) - \\mathbf{e}\\|_2^2 + \\| \\mathbf{z}_e - \\text{stop\\_gradient}(\\mathbf{e})\\|_2.\n",
    "$$\n",
    "\n",
    "Pay attention to the $\\text{stop\\_gradient}(*)$ operator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2o9DFErHqbp"
   },
   "source": [
    "Our first step is implement vector quantization procedure. It will also calculate two consistency losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1709365696135,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "crnRkktfHm2v"
   },
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_embeddings: int = 128, embedding_dim: int = 16, beta: float = 0.25\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.beta = beta\n",
    "\n",
    "        # Initialize the embeddings which we will quantize.\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)\n",
    "\n",
    "    def get_code_indices(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = x.shape[:-1]\n",
    "        flattened = x.view(-1, self.embedding_dim)\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) calculate distances from flatten inputs to embeddings\n",
    "        # 2) find nearest embeddings to each input (use argmin op)\n",
    "\n",
    "        # ====\n",
    "        encoding_indices = encoding_indices.view(input_shape)\n",
    "        return encoding_indices\n",
    "\n",
    "    def get_quantized(self, encoding_indices: torch.Tensor) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) get embeddgins with appropriate indices\n",
    "        # 2) transform tensor from BHWC to BCHW format\n",
    "        \n",
    "        # ====\n",
    "        return quantized\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) get indices\n",
    "        # 2) get quantized latents\n",
    "        # 3) calculate codebook and commitment loss\n",
    "        #    do not afraid about stop_gradient op\n",
    "        #    (use .detach() method for quantized latents and x)\n",
    "        # 4) final loss is codebook_loss + beta * commitment_loss\n",
    "\n",
    "        # ====\n",
    "\n",
    "        # Straight-through estimator (think about it!).\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        return quantized, loss\n",
    "\n",
    "\n",
    "def test_vector_quantizer():\n",
    "    x = torch.zeros((1, 16, 7, 7))\n",
    "    layer = VectorQuantizer()\n",
    "    indices = layer.get_code_indices(x)\n",
    "    assert indices.shape == (1, 7, 7)\n",
    "    quantized = layer.get_quantized(indices)\n",
    "    assert quantized.shape == (1, 16, 7, 7)\n",
    "    quantized, loss = layer(x)\n",
    "    assert quantized.shape == (1, 16, 7, 7)\n",
    "    assert loss.shape == ()\n",
    "\n",
    "\n",
    "test_vector_quantizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6R2KmkLJHt2S"
   },
   "source": [
    "We will use simple encoder/decoder with several strided convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1709365696136,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "wgin3a_nHvsM"
   },
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        # ====\n",
    "        # your code\n",
    "        # define Sequential model with Conv2d and ReLU activation\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        # ====\n",
    "        # your code\n",
    "        # define Sequential model with ConvTransposed2d and ReLU activation\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxTB_hRjHxsb"
   },
   "source": [
    "Now we are ready to define our model. It consists of encoder, decoder and vector quantizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1101,
     "status": "ok",
     "timestamp": 1709365697235,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "G6Tam1vLHz8q"
   },
   "outputs": [],
   "source": [
    "class VQVAEModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ce_loss_scale: float = 1.0,\n",
    "        latent_dim: int = 16,\n",
    "        num_embeddings: int = 64,\n",
    "        latent_size: tuple = (7, 7),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = ConvEncoder(latent_dim)\n",
    "        self.decoder = ConvDecoder(latent_dim)\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, latent_dim)\n",
    "        self.ce_loss_scale = ce_loss_scale\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply encoder\n",
    "        # 2) apply vector quantizer (it returns quantized representation + vq_loss)\n",
    "        # 3) apply decoder (it returns decoded samples)\n",
    "        \n",
    "        # ====\n",
    "        return decoded, vq_loss\n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> dict:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply model\n",
    "        # 2) get cross entropy loss\n",
    "        \n",
    "        # ====\n",
    "        return {\n",
    "            \"total_loss\": self.ce_loss_scale * ce_loss + vq_loss,\n",
    "            \"ce_loss\": self.ce_loss_scale * ce_loss,\n",
    "            \"vq_loss\": vq_loss,\n",
    "        }\n",
    "\n",
    "    def get_indices(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply encoder\n",
    "        # 2) get indices of codes using vector quantizer\n",
    "        \n",
    "        # ====\n",
    "        return codebook_indices\n",
    "\n",
    "    def prior(self, n: int) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # prior distribution is uniform\n",
    "        # 1) get samples from categorical distribution\n",
    "        # 2) get quantized representations using vector quantizer\n",
    "        \n",
    "        # ====\n",
    "        return quantized\n",
    "\n",
    "    def sample_from_logits(self, logits: torch.Tensor) -> np.ndarray:\n",
    "        # ====\n",
    "        # your code\n",
    "        # our model will return logits, this method applies softmax and samples from the distribution\n",
    "        # 1) apply softmax to the logits\n",
    "        # 2) sample from the distribution (e.x. you could use torch.multinomial)\n",
    "        # be careful with the sizes of the tensors (may be you need to permute/reshape dimensios)\n",
    "        \n",
    "        # ====\n",
    "        return samples.cpu().numpy()\n",
    "\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        with torch.no_grad():\n",
    "            # ====\n",
    "            # your code\n",
    "            # 1) sample from prior distribution\n",
    "            # 2) apply decoder\n",
    "            # 3) sample from logits\n",
    "            \n",
    "            # ====\n",
    "            return samples\n",
    "\n",
    "\n",
    "def test_vqvae_model():\n",
    "    model = VQVAEModel().cuda()\n",
    "    x = torch.zeros((2, 1, 28, 28)).cuda()\n",
    "\n",
    "    encoded = model.encoder(x)\n",
    "    size = encoded.shape[2:]\n",
    "    assert size == model.latent_size\n",
    "\n",
    "    indices = model.get_indices(x)\n",
    "    assert indices.shape == (2, 7, 7)\n",
    "\n",
    "    losses = model.loss(x)\n",
    "    assert isinstance(losses, dict)\n",
    "    assert \"total_loss\" in losses\n",
    "\n",
    "    quantized = model.prior(10)\n",
    "    assert quantized.shape == (10, 16, *model.latent_size)\n",
    "\n",
    "    decoded = model.decoder(quantized)\n",
    "    assert decoded.shape == (10, 2, 28, 28)\n",
    "\n",
    "    sampled = model.sample(10)\n",
    "    assert sampled.shape == (10, 1, 28, 28)\n",
    "\n",
    "\n",
    "test_vqvae_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MTgyRyQH14x"
   },
   "source": [
    "Let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490,
     "referenced_widgets": [
      "03dc614f4b9943ab91923025f5c5b0e0",
      "ebfed28360ad4dffab39a9d23ec8bbfc",
      "c21d4335872e463590799841ec35b1eb",
      "db1ebda23ce8424a857d3e73846fc0df",
      "1a14cbb10b9e4942baff7b389ac0c299",
      "a2d8ebaf188b481ba242ff07990b581d",
      "83fdc33f980d41c4a600531fbeab2fc0",
      "72a8540e9ce24b0f9c38a4a29c1377de",
      "303ef90204bd48e3a5423fac513cbb49",
      "c21c86b35c1c4b178d23f884add233bb",
      "cca2526aa9dc49eebd3e55e2f553e89b"
     ]
    },
    "executionInfo": {
     "elapsed": 40810,
     "status": "ok",
     "timestamp": 1709365738043,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "5pssXU7ZH3S7",
    "outputId": "b4234f9a-f357-429e-ee83-34ae7cb7810d"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE =   # any adequate value\n",
    "EPOCHS =   # < 30\n",
    "LR =   # < 1e-2\n",
    "CE_SCALE =   # 0.01 < x < 30.0\n",
    "# ====\n",
    "\n",
    "train_data, test_data = load_dataset(\"mnist\", flatten=False, binarize=True)\n",
    "\n",
    "model = VQVAEModel(ce_loss_scale=CE_SCALE, latent_dim=16, num_embeddings=128)\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_losses, test_losses = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    use_cuda=USE_CUDA,\n",
    "    use_tqdm=True,\n",
    "    lr=LR,\n",
    ")\n",
    "\n",
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TvS_fGKH5Is"
   },
   "source": [
    "Now we is able to sample from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 828,
     "status": "ok",
     "timestamp": 1709365738852,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "ebHjZdfyH6es",
    "outputId": "f3b7555b-c3dd-43da-b959-8e9c4f599f77"
   },
   "outputs": [],
   "source": [
    "# Test losses\n",
    "for key, value in test_losses.items():\n",
    "    print(\"{}: {:.4f}\".format(key, value[-1]))\n",
    "\n",
    "# Samples\n",
    "samples = model.sample(100)\n",
    "samples = samples.astype(\"float32\")\n",
    "show_samples(samples, title=\"Samples\")\n",
    "\n",
    "# Reconstructions\n",
    "x = next(iter(test_loader))[:50].cuda()\n",
    "with torch.no_grad():\n",
    "    decoded, _ = model(x)\n",
    "    x_recon = model.sample_from_logits(decoded)\n",
    "x = x.cpu().numpy()\n",
    "reconstructions = np.concatenate((x, x_recon), axis=0)\n",
    "reconstructions = reconstructions.astype(\"float32\")\n",
    "show_samples(reconstructions, title=\"Reconstructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRtQoLfWH8h_"
   },
   "source": [
    "Probably you will get bad samples :(\n",
    "\n",
    "Do not worry, may be it is OK, we will try to fix your samples! Make sure that reconstructions are almost perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Dg61P0IH-de"
   },
   "source": [
    "Here, we will visualize latent code indices for test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 608,
     "status": "ok",
     "timestamp": 1709365739458,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "IFbRkzkpH_y-",
    "outputId": "ba87e656-a01d-45b2-d647-7f8491f3e905"
   },
   "outputs": [],
   "source": [
    "test_images = next(iter(test_loader))[:100]\n",
    "x = test_images.cuda()\n",
    "codebook_indices = model.get_indices(x).cpu().unsqueeze(1)\n",
    "\n",
    "show_samples(test_images, \"Test images\")\n",
    "show_samples(codebook_indices, \"Test codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn2OJFvEIBb_"
   },
   "source": [
    "### Training of prior autoregressive model\n",
    "\n",
    "The samples from our VQ-VAE model is not good enough. The authors of the original VQ-VAE paper proposed to train autoregressive model in the latent space after we trained VQ-VAE model.\n",
    "\n",
    "Remember we have discussed **ELBO surgery** and **aggregrated posterior**. Let recall what do we have in VAE:\n",
    "* **Training:** we get latent variables $\\mathbf{z}$ from variational posterior $q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})$ for every object $\\mathbf{x}$ and then applies decoder ($p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})$). It means that in average decoder is applied to the latent variables from aggregated posterior $q_{\\text{agg}}(\\mathbf{z} | \\boldsymbol{\\phi})$.\n",
    "* **Inference:** We apply decoder to the latent variables from prior distribution $p(\\mathbf{z})$.\n",
    "\n",
    "It means that if our aggregated posterior $q_{\\text{agg}}(\\mathbf{z} | \\boldsymbol{\\phi})$ and prior $p(\\mathbf{z})$ is too far from each other, then we get inconsistency.\n",
    "\n",
    "So let train to remove this inconsistency. To be concrete, let train (autoregressive) model in the latent space that will try to predict samples from the aggregated posterior $q_{\\text{agg}}(\\mathbf{z} | \\boldsymbol{\\phi})$.\n",
    "\n",
    "We will use our good friend: PixelCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1709365739459,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "hx4pWRNhIDTE"
   },
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(\n",
    "        self, mask_type: str, in_channels: int, out_channels: int, kernel_size: int = 5\n",
    "    ) -> None:\n",
    "        assert mask_type in [\"A\", \"B\"]\n",
    "        super().__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "        )\n",
    "        self.register_buffer(\"mask\", torch.zeros_like(self.weight))\n",
    "        self.create_mask(mask_type)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def create_mask(self, mask_type: str) -> None:\n",
    "        # ====\n",
    "        # your code\n",
    "        # do not forget about mask_type\n",
    "        \n",
    "        # ====\n",
    "\n",
    "\n",
    "def test_masked_conv2d():\n",
    "    layer = MaskedConv2d(\"A\", 2, 2)\n",
    "    assert np.allclose(layer.mask[:, :, 2, 2].numpy(), np.zeros((2, 2)))\n",
    "\n",
    "    layer = MaskedConv2d(\"B\", 2, 2)\n",
    "    assert np.allclose(layer.mask[:, :, 2, 2].numpy(), np.ones((2, 2)))\n",
    "\n",
    "\n",
    "test_masked_conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1709365739459,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "gQXIckZpIEyY"
   },
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int = 128,\n",
    "        input_shape: tuple = (7, 7),\n",
    "        n_filters: int = 32,\n",
    "        kernel_size: int = 5,\n",
    "        n_layers: int = 5,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply the sequence of MaskedConv2d -> ReLU\n",
    "        # the last layer should be MaskedConv2d (not ReLU)\n",
    "        # Note 1: the first conv layer should be of type 'A'\n",
    "        # Note 2: final output_dim in MaskedConv2d must be 2\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # read the forward method carefully\n",
    "        flattened = x.view((-1, 1))\n",
    "        encodings = torch.zeros(flattened.shape[0], self.num_embeddings).cuda()\n",
    "        encodings.scatter_(1, flattened, 1)\n",
    "        encodings = encodings.view((-1, *self.input_shape, self.num_embeddings))\n",
    "        encodings = encodings.permute((0, 3, 1, 2))\n",
    "        out = self.net(encodings)\n",
    "        out = out.view(-1, self.num_embeddings, 1, *self.input_shape)\n",
    "        return out\n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> dict:\n",
    "        # ====\n",
    "        # your code\n",
    "        \n",
    "        # ====\n",
    "        return {\"total_loss\": total_loss}\n",
    "\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        # read carefully the sampling process\n",
    "        samples = torch.zeros(n, 1, *self.input_shape, dtype=torch.int64).cuda()\n",
    "        with torch.no_grad():\n",
    "            for r in range(self.input_shape[0]):\n",
    "                for c in range(self.input_shape[1]):\n",
    "                    logits = self(samples)[:, :, :, r, c]\n",
    "                    probs = F.softmax(logits, dim=1).squeeze(-1)\n",
    "                    samples[:, 0, r, c] = torch.multinomial(\n",
    "                        probs, num_samples=1\n",
    "                    ).squeeze(-1)\n",
    "        return samples.cpu().numpy()\n",
    "\n",
    "\n",
    "def test_pixelcnn():\n",
    "    model = PixelCNN().cuda()\n",
    "    x = torch.zeros((1, 1, 7, 7), dtype=torch.int64).cuda()\n",
    "    output = model(x)\n",
    "    assert output.shape == (1, 128, 1, 7, 7)\n",
    "    losses = model.loss(x)\n",
    "    assert isinstance(losses, dict)\n",
    "    assert \"total_loss\" in losses\n",
    "    samples = model.sample(10)\n",
    "    assert samples.shape == (10, 1, 7, 7)\n",
    "\n",
    "\n",
    "test_pixelcnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU2O_ZUSIGYn"
   },
   "source": [
    "Now we need to get our train and test samples. Our model will predict indices of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1709365739459,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "pzCCgD0dIHwz"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# you have to get indices of the emdeddings from the VQ-VAE model for train and test data\n",
    "INPUT_SHAPE = (7, 7)  # input shape of your latent space\n",
    "\n",
    "# ====\n",
    "\n",
    "assert isinstance(train_indices, np.ndarray)\n",
    "assert isinstance(test_indices, np.ndarray)\n",
    "assert train_indices.shape == (60000, 1, *INPUT_SHAPE)\n",
    "assert test_indices.shape == (10000, 1, *INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6edd5962dc354834993e41c0d54c94de",
      "49d624d6489f44dda136bf79960314fc",
      "b4fdcf9c7cab40319e09a91ff1a32e75",
      "5c7042457920424384aeb4656a447ef5",
      "364720107e0041bc8cc60148675a0ae9",
      "3fe0bf255eba43859f9d207646f1255c",
      "0b290cfa6bfc4cd4844200d2b088a528",
      "81a7a92d5f6a4e15a332226aab686610",
      "2feb36240afe434495d2bc45053d1103",
      "d1990dda9bd24d6692447b4258809c98",
      "6c630a6f97404188aaf853956b81d34c"
     ]
    },
    "executionInfo": {
     "elapsed": 21738,
     "status": "ok",
     "timestamp": 1709365761189,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "bRDOTXifIJHJ",
    "outputId": "c88316ec-37c1-4730-c554-1c8225e75ae6"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters by your own\n",
    "EPOCHS =   # > 5\n",
    "BATCH_SIZE =   # any adequate value\n",
    "LR =   # < 1e-2\n",
    "N_LAYERS =   # < 10\n",
    "N_FILTERS =   # < 128\n",
    "# ====\n",
    "\n",
    "prior_model = PixelCNN(\n",
    "    input_shape=INPUT_SHAPE, n_filters=N_FILTERS, kernel_size=5, n_layers=N_LAYERS\n",
    ")\n",
    "\n",
    "train_loader = data.DataLoader(train_indices, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_indices, batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(\n",
    "    prior_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    use_tqdm=True,\n",
    "    use_cuda=USE_CUDA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfgh_cRXIKio"
   },
   "source": [
    "Now we are ready to sample from our VQ-VAE model. The difference here that we will sample our embedding indices from the PixelCNN prior model instead of the Uniform prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1709365761851,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "CpAPrirKIL57",
    "outputId": "bfd1a9cc-5c01-4b11-88bf-41184b973002"
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 100\n",
    "indices = prior_model.sample(N_SAMPLES).squeeze(1)\n",
    "quantized = model.vq_layer.get_quantized(torch.Tensor(indices).int().cuda())\n",
    "logits = model.decoder(quantized)\n",
    "samples = model.sample_from_logits(logits)\n",
    "\n",
    "samples = samples.astype(\"float32\")\n",
    "show_samples(samples, title=\"Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmPH_hO0INa4"
   },
   "source": [
    "Here you have to get samples with good enough quality!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03dc614f4b9943ab91923025f5c5b0e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ebfed28360ad4dffab39a9d23ec8bbfc",
       "IPY_MODEL_c21d4335872e463590799841ec35b1eb",
       "IPY_MODEL_db1ebda23ce8424a857d3e73846fc0df"
      ],
      "layout": "IPY_MODEL_1a14cbb10b9e4942baff7b389ac0c299"
     }
    },
    "0b290cfa6bfc4cd4844200d2b088a528": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "159726388b174e5caa99596a7dba1757": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9928da49da140ee9f7c647d9c4dd25b",
      "placeholder": "​",
      "style": "IPY_MODEL_e139d57637af473e9254cdbb5da91fc2",
      "value": "100%"
     }
    },
    "1a14cbb10b9e4942baff7b389ac0c299": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2feb36240afe434495d2bc45053d1103": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "303ef90204bd48e3a5423fac513cbb49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "364720107e0041bc8cc60148675a0ae9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fe0bf255eba43859f9d207646f1255c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49d624d6489f44dda136bf79960314fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3fe0bf255eba43859f9d207646f1255c",
      "placeholder": "​",
      "style": "IPY_MODEL_0b290cfa6bfc4cd4844200d2b088a528",
      "value": "100%"
     }
    },
    "5c7042457920424384aeb4656a447ef5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1990dda9bd24d6692447b4258809c98",
      "placeholder": "​",
      "style": "IPY_MODEL_6c630a6f97404188aaf853956b81d34c",
      "value": " 5/5 [00:21&lt;00:00,  4.35s/it]"
     }
    },
    "65082ee43a844f62ac8287b8faef5b66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6c630a6f97404188aaf853956b81d34c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6edd5962dc354834993e41c0d54c94de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_49d624d6489f44dda136bf79960314fc",
       "IPY_MODEL_b4fdcf9c7cab40319e09a91ff1a32e75",
       "IPY_MODEL_5c7042457920424384aeb4656a447ef5"
      ],
      "layout": "IPY_MODEL_364720107e0041bc8cc60148675a0ae9"
     }
    },
    "72a8540e9ce24b0f9c38a4a29c1377de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78cb849d96014a108015bd3f92f2b968": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a5d33c83dac4086a8cd16b226b59dfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_159726388b174e5caa99596a7dba1757",
       "IPY_MODEL_dc7e4461995942a9855567ee63808372",
       "IPY_MODEL_93cf61378d324a7498665c969526ce92"
      ],
      "layout": "IPY_MODEL_9b97df51f0764ba09eebd9fb8cac9380"
     }
    },
    "81a7a92d5f6a4e15a332226aab686610": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83fdc33f980d41c4a600531fbeab2fc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93cf61378d324a7498665c969526ce92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4763079e73a4982a540c05cd82a00d2",
      "placeholder": "​",
      "style": "IPY_MODEL_78cb849d96014a108015bd3f92f2b968",
      "value": " 10/10 [03:41&lt;00:00, 21.82s/it]"
     }
    },
    "9b97df51f0764ba09eebd9fb8cac9380": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2d8ebaf188b481ba242ff07990b581d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4763079e73a4982a540c05cd82a00d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4fdcf9c7cab40319e09a91ff1a32e75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81a7a92d5f6a4e15a332226aab686610",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2feb36240afe434495d2bc45053d1103",
      "value": 5
     }
    },
    "c21c86b35c1c4b178d23f884add233bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c21d4335872e463590799841ec35b1eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72a8540e9ce24b0f9c38a4a29c1377de",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_303ef90204bd48e3a5423fac513cbb49",
      "value": 10
     }
    },
    "c9928da49da140ee9f7c647d9c4dd25b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cca2526aa9dc49eebd3e55e2f553e89b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1990dda9bd24d6692447b4258809c98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db1ebda23ce8424a857d3e73846fc0df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c21c86b35c1c4b178d23f884add233bb",
      "placeholder": "​",
      "style": "IPY_MODEL_cca2526aa9dc49eebd3e55e2f553e89b",
      "value": " 10/10 [00:40&lt;00:00,  3.96s/it]"
     }
    },
    "dc7e4461995942a9855567ee63808372": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc8fc8cbe5e949e6a279f2692cff54aa",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65082ee43a844f62ac8287b8faef5b66",
      "value": 10
     }
    },
    "dc8fc8cbe5e949e6a279f2692cff54aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e139d57637af473e9254cdbb5da91fc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebfed28360ad4dffab39a9d23ec8bbfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2d8ebaf188b481ba242ff07990b581d",
      "placeholder": "​",
      "style": "IPY_MODEL_83fdc33f980d41c4a600531fbeab2fc0",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
